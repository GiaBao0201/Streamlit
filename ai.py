import os
import time
import threading
from google.cloud import speech, texttospeech, vision, translate_v2 as translate
from pydub import AudioSegment
from pygame import mixer
mixer.init(frequency=44100, size=-16, channels=1, buffer=512)
import RPi.GPIO as GPIO
import pigpio
import speech_recognition as sr
import google.generativeai as genai  # ‚úÖ D√πng Gemini API

# ==== C·∫•u h√¨nh Google Cloud & Gemini ====
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/home/admin/Downloads/esp32-441714-19ff1efdb06f.json"
GEMINI_API_KEY = "YOUR_GEMINI_API_KEY"   # ‚ö†Ô∏è Thay b·∫±ng API key Gemini c·ªßa b·∫°n
genai.configure(api_key=GEMINI_API_KEY)

# ==== GPIO setup ====
CHAT_BUTTON_PIN = 26
OCR_BUTTON_PIN = 12
GPIO.setmode(GPIO.BCM)
GPIO.setup(CHAT_BUTTON_PIN, GPIO.IN, pull_up_down=GPIO.PUD_UP)
GPIO.setup(OCR_BUTTON_PIN, GPIO.IN, pull_up_down=GPIO.PUD_UP)

# ==== Mixer setup ====
mixer.init()

# ==== Bi·∫øn to√†n c·ª•c ====
is_processing = False
ocr_text_global = ""
is_paused = False
pi = pigpio.pi()
if not pi.connected:
    print("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi PIGPIO daemon.")
    exit()

# ==== TTS ====
def play_audio(text, lang='vi', output_file='output.mp3', speed=1.1):
    global is_paused
    client = texttospeech.TextToSpeechClient()
    synthesis_input = texttospeech.SynthesisInput(text=text)

    voice_params = {
        'vi': texttospeech.VoiceSelectionParams(language_code="vi-VN",
                                                ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL),
        'en': texttospeech.VoiceSelectionParams(language_code="en-US",
                                                ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL)
    }
    voice = voice_params.get(lang, voice_params['vi'])
    audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)
    response = client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)

    with open(output_file, "wb") as out:
        out.write(response.audio_content)

    sound = AudioSegment.from_file(output_file)
    sound_with_speed = sound.speedup(playback_speed=speed)
    sound_with_speed.export(output_file, format="mp3")

    mixer.music.load(output_file)
    mixer.music.play()

    while mixer.music.get_busy():
        if is_paused:
            mixer.music.pause()
            while is_paused:
                time.sleep(0.1)
            mixer.music.unpause()
        time.sleep(0.1)


# ==== Ph√°t hi·ªán ng√¥n ng·ªØ ====
def detect_language(text):
    translate_client = translate.Client()
    result = translate_client.detect_language(text)
    detected_language = result['language']
    print(f"üåê Ph√°t hi·ªán ng√¥n ng·ªØ: {detected_language}")
    return detected_language


# ==== Speech to Text ====
def setup_google_speech_to_text():
    recognizer = sr.Recognizer()
    microphone = sr.Microphone()
    with microphone as source:
        print("üéôÔ∏è ƒêang c√¢n ch·ªânh micro...")
        recognizer.adjust_for_ambient_noise(source)
        print("Micro s·∫µn s√†ng, h√£y n√≥i...")
        audio = recognizer.listen(source)
    client = speech.SpeechClient()
    audio_content = audio.get_wav_data()
    audio_wav = speech.RecognitionAudio(content=audio_content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=44100,
        language_code="vi-VN",
    )
    response = client.recognize(config=config, audio=audio_wav)
    for result in response.results:
        text = result.alternatives[0].transcript
        print(f"üó£Ô∏è B·∫°n n√≥i: {text}")
        return text


# ==== Gemini Chat ====
def ask_gemini(prompt):
    try:
        model = genai.GenerativeModel("gemini-1.5-flash")  # ‚ö° Model nhanh v√† r·∫ª
        response = model.generate_content(prompt)
        return response.text.strip() if response and response.text else None
    except Exception as e:
        print("‚ùå L·ªói khi g·ªçi Gemini API:", e)
        return None


# ==== X·ª≠ l√Ω Chat ====
def handle_chat_interaction():
    global ocr_text_global
    print("üß† B·∫Øt ƒë·∫ßu tr√≤ chuy·ªán v·ªõi Gemini...")
    play_audio("Xin ch√†o, t√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?")
    try:
        speech_text = setup_google_speech_to_text()
        if not speech_text:
            play_audio("T√¥i ch∆∞a nghe r√µ. B·∫°n c√≥ th·ªÉ n√≥i l·∫°i kh√¥ng?")
            return
        print(f"B·∫°n n√≥i: {speech_text}")
        combined_prompt = f"N·ªôi dung OCR: {ocr_text_global}\nC√¢u h·ªèi: {speech_text}"
        response = ask_gemini(combined_prompt)
        if not response:
            play_audio("Xin l·ªói, t√¥i ch∆∞a th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.")
        else:
            print(f"ü§ñ Gemini tr·∫£ l·ªùi: {response}")
            lang = detect_language(response)
            play_audio(response, lang)
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói trong qu√° tr√¨nh chat: {e}")
        play_audio("ƒê√£ x·∫£y ra l·ªói, vui l√≤ng th·ª≠ l·∫°i.")


# ==== OCR ====
def take_picture():
    return os.system('rpicam-jpeg --output test.jpg') == 0


def perform_ocr(image_path):
    client = vision.ImageAnnotatorClient()
    with open(image_path, 'rb') as image_file:
        content = image_file.read()
    image = vision.Image(content=content)
    response = client.text_detection(image=image)
    texts = response.text_annotations
    if texts:
        return texts[0].description
    return ""


def handle_ocr_and_tts():
    global is_processing, ocr_text_global
    is_processing = True
    if take_picture():
        play_audio("ƒê√£ s·∫µn s√†ng.")
        ocr_text = perform_ocr("test.jpg")
        if ocr_text:
            print(f"üìñ OCR n·ªôi dung: {ocr_text}")
            ocr_text_global = ocr_text
            lang = detect_language(ocr_text)
            play_audio(ocr_text, lang)
        else:
            play_audio("Kh√¥ng th·ªÉ ƒë·ªçc ƒë∆∞·ª£c vƒÉn b·∫£n, vui l√≤ng th·ª≠ l·∫°i.")
    else:
        play_audio("Camera ch∆∞a s·∫µn s√†ng.")
    is_processing = False


# ==== Main Loop ====
def main():
    print("üöÄ AI Vision Reader (Gemini Edition) ƒë√£ kh·ªüi ƒë·ªông.")
    last_chat_state = pi.read(CHAT_BUTTON_PIN)
    try:
        while True:
            chat_button_state = pi.read(CHAT_BUTTON_PIN)
            ocr_button_state = GPIO.input(OCR_BUTTON_PIN)

            if chat_button_state == 0 and last_chat_state == 1:
                handle_chat_interaction()

            if ocr_button_state == GPIO.LOW and not is_processing:
                print("üñºÔ∏è B·∫Øt ƒë·∫ßu OCR...")
                threading.Thread(target=handle_ocr_and_tts).start()

            last_chat_state = chat_button_state
            time.sleep(0.05)
    except KeyboardInterrupt:
        print("üõë ƒê√£ d·ª´ng AI Vision Reader.")
    finally:
        mixer.quit()
        GPIO.cleanup()
        pi.stop()


if __name__ == '__main__':
    main()
